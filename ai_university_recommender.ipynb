{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPo6WjvxyqAqt9v1D8HH8XO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajShah3006/university-recommender-ai/blob/main/ai_university_recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cad82c67"
      },
      "source": [
        "!pip install -q google-generativeai"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f7c4f31"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "# Assume GOOGLE_API_KEY is already set up in Colab secrets\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Initialize the Gemini API\n",
        "model = genai.GenerativeModel('models/gemini-2.5-flash-preview-05-20')\n",
        "chat = model.start_chat(history=[])\n",
        "\n",
        "def get_user_information():\n",
        "    \"\"\"Collects information from the user.\"\"\"\n",
        "    user_data = {}\n",
        "    user_data['subjects'] = input(\"What subjects are you taking currently? \")\n",
        "    user_data['intrests'] = input(\"What are your intrests, some activity that you love to put effort into or you would like to be doing in 4 years? \")\n",
        "    user_data['overall_average'] = input(\"What is your overall average? \")\n",
        "    user_data['grade'] = input(\"What grade are you in? \")\n",
        "    user_data['location'] = input(\"Where are you located? \")\n",
        "    return user_data\n",
        "\n",
        "def list_all_programs(url):\n",
        "    \"\"\"\n",
        "    Scrapes and lists all program names and their URLs from the given URL.\n",
        "\n",
        "    Args:\n",
        "      url: The URL of the page listing all programs.\n",
        "\n",
        "    Returns:\n",
        "      A list of dictionaries, where each dictionary contains 'name' and 'url'\n",
        "      for a program, or None if scraping fails or no programs are found.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        programs_list = []\n",
        "\n",
        "        # Find the main container\n",
        "        container = soup.select_one('div.results.results-programs')\n",
        "\n",
        "        if not container:\n",
        "            print(f\"Could not find the main programs container on {url}\")\n",
        "            return None\n",
        "\n",
        "        # Find all program title elements within the container\n",
        "        program_elements = container.select('h2.result-heading')\n",
        "\n",
        "        if not program_elements:\n",
        "            print(f\"No program title elements found within the container on {url}\")\n",
        "            return None\n",
        "\n",
        "        # Extract program names and URLs\n",
        "        for program_element in program_elements:\n",
        "            program_name = program_element.get_text(strip=True)\n",
        "            anchor_tag = program_element.find('a', href=True)\n",
        "            if anchor_tag:\n",
        "                program_url = anchor_tag['href']\n",
        "                programs_list.append({'name': program_name, 'url': program_url})\n",
        "\n",
        "        if programs_list:\n",
        "            return programs_list\n",
        "        else:\n",
        "            print(f\"No program names and URLs extracted from {url}\")\n",
        "            return None\n",
        "\n",
        "    except requests.exceptions.Timeout:\n",
        "        print(f\"Request timed out for URL: {url}\")\n",
        "        return None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching URL {url}: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during scraping or parsing {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def scrape_university_info(url):\n",
        "    \"\"\"\n",
        "    Scrapes university information from a given URL with a more flexible approach.\n",
        "\n",
        "    Args:\n",
        "      url: https://www.ouinfo.ca/\n",
        "\n",
        "    Returns:\n",
        "      A dictionary containing the extracted university information, or None if\n",
        "      scraping fails or no information is found.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        university_data = {}\n",
        "\n",
        "        # --- Flexible Searching for Information ---\n",
        "\n",
        "        # Try to find the program title\n",
        "        program_element = soup.select_one('h1.program-title')\n",
        "        if program_element:\n",
        "            university_data['program'] = program_element.get_text(strip=True)\n",
        "\n",
        "        # Search for prerequisites using keywords and nearby list structures\n",
        "        prerequisites = []\n",
        "        # Look for common headings or text near prerequisites\n",
        "        prereq_headings = soup.find_all(string=re.compile(r'Prerequisites|Admission Requirements', re.IGNORECASE))\n",
        "        for heading in prereq_headings:\n",
        "            # Try to find a list (ul or ol) immediately following the heading or within the same parent element\n",
        "            list_element = heading.find_next(['ul', 'ol'])\n",
        "            if list_element:\n",
        "                list_items = [li.get_text(strip=True) for li in list_element.select('li')]\n",
        "                if list_items:\n",
        "                    prerequisites.extend(list_items)\n",
        "            else:\n",
        "                # If no list is found, try to extract text from the parent element or nearby paragraphs\n",
        "                parent = heading.parent\n",
        "                if parent:\n",
        "                    # Look for text in the parent or next siblings that might contain prerequisites\n",
        "                    text_content = parent.get_text(separator=' ', strip=True)\n",
        "                    if len(text_content) > len(heading.get_text(strip=True)):  # Basic check to avoid just getting the heading text\n",
        "                        prerequisites.append(text_content)\n",
        "\n",
        "        if prerequisites:\n",
        "            # Join unique prerequisites with newlines\n",
        "            university_data['prerequisites'] = \"\\n\".join(list(set(prerequisites)))\n",
        "\n",
        "        # Search for admission average using keywords and patterns\n",
        "        admission_average = None\n",
        "        # Look for text containing keywords like \"average\", \"admission\", \"minimum\" followed by percentages or ranges\n",
        "        average_text = soup.find(string=re.compile(r'(?:admission|minimum)?\\s*average.*?\\d+%', re.IGNORECASE))\n",
        "        if average_text:\n",
        "            admission_average = average_text.strip()\n",
        "        else:\n",
        "            # Look for common classes or structures near average information\n",
        "            average_element = soup.select_one('.admission-average-range, .average-grade')  # Add other potential classes\n",
        "            if average_element:\n",
        "                admission_average = average_element.get_text(strip=True)\n",
        "\n",
        "        if admission_average:\n",
        "            university_data['admission_average'] = admission_average\n",
        "\n",
        "        # Location is still unlikely to be on this page, keeping as None for now\n",
        "        location_element = None\n",
        "        if location_element:\n",
        "            university_data['location'] = location_element.get_text(strip=True)\n",
        "\n",
        "        if university_data:\n",
        "            return university_data\n",
        "        else:\n",
        "            # print(f\"No relevant information found on {url}\") # Suppress this for cleaner output during bulk scraping\n",
        "            return None\n",
        "\n",
        "    except requests.exceptions.Timeout:\n",
        "        print(f\"Request timed out for URL: {url}\")\n",
        "        return None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching URL {url}: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during scraping or parsing {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def generate_chatbot_response(user_data, all_programs_detailed_data):\n",
        "    \"\"\"Generates a chatbot response based on user data and scraped university info.\"\"\"\n",
        "    prompt = f\"\"\"Based on the following student information:\n",
        "- **Subjects:** {user_data['subjects']}\n",
        "- **Intrests:** {user_data['intrests']}\n",
        "- **Overall Average:** {user_data['overall_average']}\n",
        "- **Grade:** {user_data['grade']}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    relevant_programs_info = \"\"\n",
        "    user_interests_keywords = user_data['intrests'].lower().split() # Split interests into keywords\n",
        "\n",
        "    if all_programs_detailed_data:\n",
        "        relevant_programs_info += \"\\n**Information about potentially relevant programs:**\\n\\n\" # Added introductory sentence and bolding\n",
        "        relevant_programs = []\n",
        "        for program_data in all_programs_detailed_data:\n",
        "            program_name_lower = program_data.get('program_name', '').lower()\n",
        "            # Enhanced filtering: Check if any interest keyword is a substring of a word in the program name\n",
        "            # or if the program name is a substring of an interest keyword.\n",
        "            if any(keyword in program_name_lower.split() or program_name_lower in keyword for keyword in user_interests_keywords):\n",
        "                relevant_programs.append(program_data)\n",
        "\n",
        "        if relevant_programs:\n",
        "            for program_data in relevant_programs:\n",
        "                relevant_programs_info += f\"**Program Name:** {program_data.get('program_name', 'N/A')}\\n\" # Bold program name\n",
        "                relevant_programs_info += f\"**Program URL:** {program_data.get('program_url', 'N/A')}\\n\" # Bold program URL\n",
        "                relevant_programs_info += f\"**Prerequisites:** {program_data.get('prerequisites', 'N/A')}\\n\" # Bold prerequisites\n",
        "                relevant_programs_info += f\"**Admission Average:** {program_data.get('admission_average', 'N/A')}\\n\" # Bold admission average\n",
        "                relevant_programs_info += \"---\\n\\n\" # Added newline for better separation\n",
        "\n",
        "        else:\n",
        "            relevant_programs_info += \"Could not find detailed information for programs closely related to your interests.\\n\\n\"\n",
        "\n",
        "    if relevant_programs_info:\n",
        "        prompt += relevant_programs_info\n",
        "    else:\n",
        "        prompt += \"\\nCould not retrieve detailed program information.\\n\"\n",
        "\n",
        "    prompt += f\"\"\"\n",
        "Please provide some relevant information, such as:\n",
        "- What program is recommended based on the student's interests and scraped program data\n",
        "- A ranking of relevant universities for that specific program, including:\n",
        "  - What are the prerequisites\n",
        "  - Last few years admission average\n",
        "  - How far the university is located (if available from scraping or inferable)\n",
        "  - Tuition and Fees (if available from scraping or inferable)\n",
        "  - Has a supplementary application or not (if available from scraping or inferable)\n",
        "- Recommendations for high school courses to pursue and projects to complete for university applications, specifically tailored to the student's interests, current subjects ({user_data['subjects']}), and grade level ({user_data['grade']}).\n",
        "\n",
        "Be specific and tailor the response to the student's input and the provided program information. Only give information for universities in Ontario.\n",
        "\"\"\"\n",
        "    response = chat.send_message(prompt)\n",
        "    return response.text\n",
        "\n",
        "# --- Main Execution Flow ---\n",
        "print(\"Hello! I'm a student assistant chatbot. I can help you with information related to your studies.\")\n",
        "\n",
        "# 1. Get user information\n",
        "student_info = get_user_information()\n",
        "\n",
        "# 2. Scrape program URLs\n",
        "base_url = \"https://www.ouinfo.ca\"\n",
        "programs_with_urls = list_all_programs(f\"{base_url}/programs/all\")\n",
        "\n",
        "all_programs_detailed_data = []\n",
        "\n",
        "# 3. Scrape detailed data for each program\n",
        "if programs_with_urls:\n",
        "    print(\"\\nScraping detailed program information...\")\n",
        "    for program in programs_with_urls:\n",
        "        program_url = f\"{base_url}{program['url']}\"\n",
        "        # print(f\"Attempting to scrape: {program_url}\") # Keep this commented for cleaner output\n",
        "        scraped_data = scrape_university_info(program_url)\n",
        "        if scraped_data:\n",
        "            # Combine program name and URL with scraped data\n",
        "            detailed_data = {\n",
        "                'program_name': program['name'],\n",
        "                'program_url': program_url,\n",
        "                **scraped_data\n",
        "            }\n",
        "            all_programs_detailed_data.append(detailed_data)\n",
        "            # print(f\"Successfully scraped data for {program['name']}\") # Keep this commented for cleaner output\n",
        "        else:\n",
        "            pass # print(f\"Failed to scrape data for {program['name']} from {program_url}\") # Keep this commented for cleaner output\n",
        "\n",
        "    print(\"Finished scraping.\")\n",
        "\n",
        "    # 4. Generate and display chatbot response\n",
        "    if all_programs_detailed_data:\n",
        "        bot_response = generate_chatbot_response(student_info, all_programs_detailed_data)\n",
        "        print(\"\\nChatbot Response:\")\n",
        "        print(bot_response)\n",
        "    else:\n",
        "        print(\"\\nNo detailed program data was successfully scraped to generate a comprehensive response.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nFailed to retrieve the initial list of programs with URLs. Cannot proceed with scraping or generating a response.\")\n",
        "\n",
        "print(\"\\nChat session ended.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}